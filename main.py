import time
import re
import os
import traceback
from datetime import datetime, timedelta
import pytz
from ics import Calendar, Event

# Selenium ç›¸å…³
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

def get_driver():
    options = Options()
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--remote-debugging-port=9222")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    chrome_binary_path = os.environ.get("CHROME_PATH")
    if chrome_binary_path:
        options.binary_location = chrome_binary_path

    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        return driver
    except Exception as e:
        print(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def clean_text_list(text_str):
    """è¾…åŠ©å‡½æ•°ï¼šå°† 'aaa|bbb||ccc' æ¸…æ´—ä¸º ['aaa', 'bbb', 'ccc']"""
    if not text_str:
        return []
    # åˆ†å‰²å¹¶å»é™¤ç©ºç™½é¡¹
    return [x.strip() for x in text_str.split('|') if x.strip()]

def parse_day_content(html_content, current_date):
    """
    ç»“æ„åŒ–è§£æï¼šæå–è¡¨æ ¼å½¢å¼çš„ç»æµæ•°æ®å’Œè´¢ç»å¤§äº‹
    """
    events = []
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„å¤§æ¿å—å®¹å™¨
    # é‡‘åçš„ç»“æ„é€šå¸¸æ˜¯ headers è·Ÿç€ content
    # æˆ‘ä»¬ä½¿ç”¨ get_text('|') æ¥ä¿ç•™åˆ—ç»“æ„
    raw_lines = soup.get_text("|", strip=True).split("|")
    
    # é‡ç»„ lineï¼šå› ä¸º get_text('|') ä¼šæŠŠä¸€è¡Œæ‹†å¾—å¾ˆç¢ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®è§†è§‰ä¸Šçš„â€œè¡Œâ€æ¥é‡ç»„
    # ä½†é‡‘åçš„å¸ƒå±€æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œé‡‡ç”¨åŸºäºâ€œæ¿å—å®šä½â€+â€œè¡Œæ‰«æâ€çš„ç­–ç•¥
    
    # é‡æ–°è·å– HTML å—è¿›è¡Œç²¾ç»†å¤„ç†
    # å¯»æ‰¾åŒ…å«â€œç»æµæ•°æ®ä¸€è§ˆâ€çš„çˆ¶çº§å®¹å™¨
    # é‡‘åé¡µé¢é€šå¸¸æœ‰æ˜ç¡®çš„ classï¼Œä½†æ··æ·†ä¸¥é‡ã€‚æˆ‘ä»¬éå†æ‰€æœ‰ div è¡Œã€‚
    
    # --- ç­–ç•¥æ›´æ–°ï¼šé’ˆå¯¹è¡Œçš„éå† ---
    # æˆ‘ä»¬æŠŠ HTML é‡Œçš„æ¯ä¸€è¡Œ (div/tr) æ‹¿å‡ºæ¥å•ç‹¬å¤„ç†
    rows = soup.find_all(['div', 'tr']) 
    
    current_section = None # 'DATA' or 'EVENT'
    
    processed_texts = set() # é˜²æ­¢é‡å¤æ·»åŠ åŒ…å«å…³ç³»çš„ div

    print(f"  æ­£åœ¨åˆ†æ {current_date} ...")

    for row in rows:
        # è·å–è¯¥è¡Œçš„æ–‡æœ¬åˆ—è¡¨ (ä¿ç•™åˆ—åˆ†éš”)
        row_text_str = row.get_text("|", strip=True)
        # å¦‚æœè¿™è¡Œæ–‡æœ¬å·²ç»è¢«åŒ…å«åœ¨æ›´å¤§çš„çˆ¶çº§é‡Œå¤„ç†è¿‡ï¼Œè·³è¿‡ (ç®€å•çš„å»é‡)
        # (å®é™…æ“ä½œä¸­ï¼Œå®Œå…¨å»é‡è¾ƒéš¾ï¼Œæˆ‘ä»¬é€šè¿‡ç‰¹å¾è¯†åˆ«æ¥è¿‡æ»¤)
        
        cols = clean_text_list(row_text_str)
        if not cols: continue
        
        full_line_text = "".join(cols)

        # 1. è¯†åˆ«æ¿å—å¤´
        if "ç»æµæ•°æ®ä¸€è§ˆ" in full_line_text and len(cols) < 5:
            current_section = 'DATA'
            continue
        if "è´¢ç»å¤§äº‹ä¸€è§ˆ" in full_line_text and len(cols) < 5:
            current_section = 'EVENT'
            continue
        if "æœŸè´§æ—¥å†" in full_line_text or "ä¼‘å¸‚æ—¥å†" in full_line_text:
            current_section = None
            continue

        if current_section is None:
            continue

        # 2. è¯†åˆ«æ•°æ®è¡Œ
        # ç‰¹å¾ï¼šç¬¬ä¸€åˆ—é€šå¸¸æ˜¯æ—¶é—´ HH:MM
        time_col = cols[0]
        if not re.match(r'^\d{2}:\d{2}$', time_col):
            continue
        
        # å†æ¬¡æ£€æŸ¥ï¼šé˜²æ­¢æŠ“å–åˆ°è¡¨å¤´ï¼ˆæ—¶é—´ã€å‰å€¼ã€é¢„æµ‹å€¼...ï¼‰
        if "å‰å€¼" in full_line_text or "é¢„æµ‹å€¼" in full_line_text:
            continue
            
        # é˜²æ­¢é‡å¤ï¼šé‡‘åçš„ DOM ç»“æ„åµŒå¥—å¾ˆæ·±ï¼Œä¸€ä¸ª row å¯èƒ½è¢« find_all æ‰¾åˆ°å¤šæ¬¡
        # æˆ‘ä»¬ç”¨æ•´è¡Œçš„ hash æ¥ç®€å•å»é‡
        row_hash = hash(full_line_text)
        if row_hash in processed_texts:
            continue
        processed_texts.add(row_hash)

        # --- 3. è§£æ [ç»æµæ•°æ®] ---
        if current_section == 'DATA':
            # å…¸å‹ç»“æ„: Time | Country | Name | Importance(maybe empty) | Actual | Forecast | Previous
            # ä½†æ˜¯åˆ—æ•°ä¸å›ºå®š (å‘å¸ƒå‰/å‘å¸ƒåä¸åŒ)
            # æˆ‘ä»¬ä»ä¸¤å¤´å¾€ä¸­é—´å‡‘
            
            event_time = cols[0]
            country = cols[1] if len(cols) > 1 else "å…¨çƒ"
            
            # æŒ‡æ ‡åç§°é€šå¸¸æ˜¯æ¯”è¾ƒé•¿çš„é‚£ä¸€æ®µ
            name = cols[2] if len(cols) > 2 else "æœªçŸ¥æŒ‡æ ‡"
            
            # å°è¯•æå–æ•°å€¼ï¼Œæ•°å€¼é€šå¸¸åœ¨æœ«å°¾ï¼Œä¸”åŒ…å«æ•°å­—ã€%ã€Bã€Mã€K
            values = []
            for col in reversed(cols):
                # å¦‚æœåŒ…å«æ•°å­—æˆ–è€…æ˜¯ "--"
                if re.search(r'\d|--', col) and len(col) < 15:
                    values.append(col)
                else:
                    # ä¸€æ—¦é‡åˆ°éæ•°å€¼ï¼ˆæ¯”å¦‚æŒ‡æ ‡åï¼‰ï¼Œå°±åœæ­¢å€’åºæŸ¥æ‰¾
                    if len(values) >= 3: # é€šå¸¸æœ€å¤š3ä¸ªæ•°å€¼ (å…¬å¸ƒ, é¢„æµ‹, å‰å€¼)
                        break
            
            # å€’åºå›æ¥çš„ï¼Œæ‰€ä»¥è¦åè½¬å›å»: [å‰å€¼, é¢„æµ‹, å…¬å¸ƒ]
            # ä½†é‡‘åçš„é¡ºåºé€šå¸¸æ˜¯: å‰å€¼ | é¢„æµ‹å€¼ | å…¬å¸ƒå€¼ (æˆ–è€…å¸ƒå±€é¡ºåºä¸åŒ)
            # ç½‘é¡µè§†è§‰é¡ºåºé€šå¸¸æ˜¯: æŒ‡æ ‡ ... å‰å€¼ é¢„æµ‹ å…¬å¸ƒ
            # æå–åˆ°çš„ values åˆ—è¡¨ç°åœ¨æ˜¯å€’åºçš„ [å…¬å¸ƒ, é¢„æµ‹, å‰å€¼]
            
            prev = "--"
            forecast = "--"
            actual = "--"
            
            if len(values) >= 1: actual = values[0]
            if len(values) >= 2: forecast = values[1]
            if len(values) >= 3: prev = values[2]
            
            # æ„å»ºäº‹ä»¶
            evt = Event()
            evt.name = f"ğŸ‡ºğŸ‡³[{country}] {name}"
            
            # è®¾ç½®æ—¶é—´
            hm = event_time.split(':')
            start_dt = datetime(
                current_date.year, current_date.month, current_date.day,
                int(hm[0]), int(hm[1]), tzinfo=pytz.timezone('Asia/Shanghai')
            )
            evt.begin = start_dt
            evt.duration = timedelta(minutes=15)
            
            evt.description = (
                f"ã€ç»æµæ•°æ®ã€‘\n"
                f"å›½å®¶/åœ°åŒº: {country}\n"
                f"æŒ‡æ ‡åç§°: {name}\n"
                f"------------------\n"
                f"å‰å€¼: {prev}\n"
                f"é¢„æµ‹: {forecast}\n"
                f"å…¬å¸ƒ: {actual}\n"
            )
            events.append(evt)
            print(f"    [æ•°æ®] {event_time} {name}")

        # --- 4. è§£æ [è´¢ç»å¤§äº‹] ---
        elif current_section == 'EVENT':
            # å…¸å‹ç»“æ„: Time | Country | City/Person | Event Content
            event_time = cols[0]
            country = cols[1] if len(cols) > 1 else ""
            
            # å‰©ä¸‹çš„åˆå¹¶ä¸ºäº‹ä»¶å†…å®¹
            content_parts = cols[2:]
            content = " ".join(content_parts)
            
            evt = Event()
            evt.name = f"ğŸ“¢[{country}] {content[:15]}..." # æ ‡é¢˜ä¸å®œå¤ªé•¿
            
            hm = event_time.split(':')
            start_dt = datetime(
                current_date.year, current_date.month, current_date.day,
                int(hm[0]), int(hm[1]), tzinfo=pytz.timezone('Asia/Shanghai')
            )
            evt.begin = start_dt
            evt.duration = timedelta(minutes=30)
            
            evt.description = (
                f"ã€è´¢ç»å¤§äº‹ã€‘\n"
                f"å›½å®¶: {country}\n"
                f"æ—¶é—´: {event_time}\n"
                f"äº‹ä»¶: {content}\n"
            )
            events.append(evt)
            print(f"    [å¤§äº‹] {event_time} {content[:10]}")

    return events

def run_scraper():
    cal = Calendar()
    driver = get_driver()
    if not driver:
        exit(1)

    try:
        base_url = "https://qihuo.jin10.com/calendar.html#/"
        today = datetime.now(pytz.timezone('Asia/Shanghai')).date()
        
        # æŠ“å–æœªæ¥ 7 å¤©
        days_to_scrape = 7
        total_count = 0

        for i in range(days_to_scrape):
            target_date = today + timedelta(days=i)
            date_str = target_date.strftime('%Y-%m-%d')
            full_url = f"{base_url}{date_str}"
            
            print(f"\n[{i+1}/{days_to_scrape}] è§£æé¡µé¢: {full_url}")
            
            try:
                driver.get(full_url)
                time.sleep(5) # ç­‰å¾…åŠ è½½
                
                html = driver.page_source
                day_events = parse_day_content(html, target_date)
                
                for e in day_events:
                    cal.events.add(e)
                    total_count += 1
                
            except Exception as e:
                print(f"    ! è§£æå‡ºé”™: {e}")

    except Exception as e:
        print(f"å…¨å±€é”™è¯¯: {traceback.format_exc()}")
    finally:
        driver.quit()

    # ä¿å­˜æ–‡ä»¶
    if total_count > 0:
        output_file = 'economic_calendar.ics'
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(cal.serialize())
        print(f"\næˆåŠŸç”Ÿæˆ {output_file}ï¼ŒåŒ…å« {total_count} ä¸ªç»“æ„åŒ–æ•°æ®ã€‚")
    else:
        print("\næœªè·å–åˆ°æ•°æ®ã€‚")

if __name__ == "__main__":
    run_scraper()
